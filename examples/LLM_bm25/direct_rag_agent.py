#!/usr/bin/env python3
"""
ç›´æ¥ä½¿ç”¨100ä¸ªWikipediaæ–‡æ¡£çš„RAG Agent
ä¸éœ€è¦åŠ¨æ€è·å–Wikipediaå†…å®¹ï¼Œç›´æ¥ä½¿ç”¨é¢„åŠ è½½çš„æ–‡æ¡£åº“
"""

import json
import tempfile
import subprocess
import os
import time
from typing import List, Tuple
from tinyllama_interface import TinyLlamaInterface
from approxMLIR import ApproxMLIRSDK
from paths import TUNER, ROOT, BENCH
SDK: ApproxMLIRSDK = ApproxMLIRSDK("./binary", "./MLIR", ROOT)

def check_answer_correct(generated_answer: str, expected_answers: List[str]) -> bool:
    """
    æ£€æŸ¥ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦åŒ…å«æœŸæœ›çš„ç­”æ¡ˆ

    Args:
        generated_answer: ç”Ÿæˆçš„ç­”æ¡ˆ
        expected_answers: æœŸæœ›çš„ç­”æ¡ˆåˆ—è¡¨

    Returns:
        æ˜¯å¦åŒ…å«æœŸæœ›ç­”æ¡ˆ
    """
    generated_lower = generated_answer.lower()

    for expected in expected_answers:
        if expected.lower() in generated_lower:
            return True

    return False


class DirectBM25Retriever:
    """ç›´æ¥ä½¿ç”¨é¢„åŠ è½½æ–‡æ¡£çš„BM25æ£€ç´¢å™¨"""

    def __init__(self, bm25_binary_path: str = None):
        self.bm25_binary_path = bm25_binary_path or os.path.abspath("bm25_file")
        self.documents = []
        self.document_ids = []
        self.document_titles = []
        self.is_loaded = False

    def load_documents_from_json(self, json_file_path: str):
        """ä»JSONæ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        print(f"Loading documents from {json_file_path}...")

        with open(json_file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        docs = data["documents"]
        self.documents = [doc["content"] for doc in docs]
        self.document_titles = [doc["title"] for doc in docs]
        self.document_ids = [f"doc_{i}" for i in range(len(docs))]
        self.is_loaded = True

        print(f"âœ… Loaded {len(self.documents)} documents")
        print(f"ğŸ“Š Total characters: {sum(len(doc) for doc in self.documents):,}")
        print(
            f"ğŸ“Š Average length: {sum(len(doc) for doc in self.documents) // len(self.documents):,} chars/doc"
        )

    def retrieve(self, query: str, top_k: int = 5) -> List[Tuple[str, float, str]]:
        """
        æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡

        Returns:
            List of tuples (document_id, score, document_text)
        """
        if not self.is_loaded:
            raise ValueError(
                "Documents not loaded. Call load_documents_from_json() first."
            )

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜å‚¨æ–‡æ¡£
        with tempfile.NamedTemporaryFile(
            mode="w", delete=False, suffix=".txt"
        ) as temp_file:
            for doc in self.documents:
                temp_file.write(doc + "\n")
            temp_file_path = temp_file.name

        # æ‰“å°è¾“å…¥ä¿¡æ¯
        print(f"ğŸ” Query: {query}")
        print(f"ğŸ“ Temp file: {temp_file_path}")
        print(f"ğŸ“Š Temp file size: {os.path.getsize(temp_file_path)} bytes")
        print(f"ğŸ“„ Temp file lines: {sum(1 for line in open(temp_file_path))} lines")

        # è°ƒç”¨BM25 Cç¨‹åº
        cmd = [self.bm25_binary_path, temp_file_path, query]
        print(f"ğŸš€ Command: {' '.join(cmd)}")

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=120,
            encoding="utf-8",
            errors="ignore",
        )

        if result.returncode != 0:
            raise RuntimeError(f"BM25 C program failed: {result.stderr}")

        # è§£æè¾“å‡º
        retrieved_docs = self._parse_bm25_output(result.stdout, top_k)

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
            print(f"ğŸ—‘ï¸  Temp file deleted: {temp_file_path}")

        return retrieved_docs

    def _parse_bm25_output(
        self, output: str, top_k: int
    ) -> List[Tuple[str, float, str]]:
        """è§£æBM25è¾“å‡º"""
        lines = output.strip().split("\n")
        retrieved_docs = []

        for line in lines:
            if line.startswith("Rank ") and "Doc " in line:
                # è§£ææ ¼å¼: "Rank 1: Doc 0 (Score: 2.5664) - "content""
                # æå–æ–‡æ¡£ç´¢å¼•
                doc_start = line.find("Doc ") + 4
                doc_end = line.find(" (Score:")
                doc_index = int(line[doc_start:doc_end])

                # æå–åˆ†æ•°
                score_start = line.find("Score: ") + 7
                score_end = line.find(") -")
                score = float(line[score_start:score_end])

                # æå–å†…å®¹
                content_start = line.find(') - "') + 5
                content_end = line.rfind('"')
                content = line[content_start:content_end]

                # è·å–æ–‡æ¡£æ ‡é¢˜
                if doc_index < len(self.document_titles):
                    title = self.document_titles[doc_index]
                    doc_id = f"doc_{doc_index}"
                    retrieved_docs.append((doc_id, score, content, title))

        return retrieved_docs[:top_k]


class DirectRAGAgent:

    def __init__(
        self,
        documents_json_path: str,
        bm25_binary_path: str = None,
    ):
        self.documents_json_path = documents_json_path
        self.bm25_binary_path = bm25_binary_path or os.path.abspath("/binary/bm25.exec")

        # åˆå§‹åŒ–ç»„ä»¶
        self.retriever = DirectBM25Retriever(self.bm25_binary_path)

        # åˆå§‹åŒ–LLMæ¥å£ï¼ˆä¸ä½¿ç”¨åŠ¨æ€é€‰æ‹©ï¼‰
        self.llm = TinyLlamaInterface()

        # å®šä¹‰æ¨¡å‹æ˜ å°„
        self.model_map = {
            1: "gemma3:270m",  # Gemma3 270M (292MB, smaller model)
            2: "gemma3:1b",  # Gemma3 1B (815MB, larger model)
        }

        # åŠ è½½æ–‡æ¡£
        self.retriever.load_documents_from_json(documents_json_path)

        print("âœ… Direct RAG Agent initialized successfully!")
        print(f"ğŸ¤– Available models: {list(self.model_map.values())}")

    def select_model_based_on_scores(self, avg_score: float) -> str:
        """
        æ ¹æ®BM25å¹³å‡åˆ†æ•°é€‰æ‹©æ¨¡å‹

        Args:
            avg_score: BM25æ£€ç´¢æ–‡æ¡£çš„å¹³å‡åˆ†æ•°

        Returns:
            é€‰æ‹©çš„æ¨¡å‹åç§°
        """

        # å°†å¹³å‡åˆ†æ•°è½¬æ¢ä¸ºæ•´æ•°ï¼ˆä¹˜ä»¥1000ä¿ç•™3ä½å°æ•°ç²¾åº¦ï¼‰
        input_score = int(avg_score * 1000)

        model_id = SDK.get_knob_val(1, input_score)
        selected_model = self.model_map.get(model_id, self.model_map[2])

        return self.model_map[2]  

    def answer_question(self, question: str, top_k: int = 10) -> dict:
        """
        å›ç­”é—®é¢˜

        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡

        Returns:
            åŒ…å«ç­”æ¡ˆå’Œæ£€ç´¢ä¿¡æ¯çš„å­—å…¸
        """
        print(f"\nğŸ” Processing question: {question}")

        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        print("ğŸ“š Retrieving relevant documents...")
        bm25_start_time = time.time()
        retrieved_docs = self.retriever.retrieve(question, top_k)
        bm25_time = time.time() - bm25_start_time

        if not retrieved_docs:
            return {
                "question": question,
                "answer": "No relevant documents found.",
                "retrieved_docs": [],
                "retrieval_count": 0,
            }

        print(f"âœ… Retrieved {len(retrieved_docs)} documents:")
        for i, (doc_id, score, content, title) in enumerate(retrieved_docs):
            print(f"  {i+1}. {title} (Score: {score:.4f})")

        # 2. è®¡ç®—BM25å¹³å‡åˆ†æ•°å¹¶é€‰æ‹©æ¨¡å‹
        avg_score = sum(score for _, score, _, _ in retrieved_docs) / len(
            retrieved_docs
        )
        selected_model = self.select_model_based_on_scores(avg_score)

        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join(
            [
                f"Document {i+1}: {content}"
                for i, (_, _, content, _) in enumerate(retrieved_docs)
            ]
        )

        print(f"ğŸ¤– Generating answer with {selected_model}...")
        start_time = time.time()
        #answer = self.llm.generate_answer(selected_model, question, retrieved_docs)
        answer = "sss"
        llm_generation_time = time.time() - start_time

        return {
            "question": question,
            "answer": answer,
            "retrieved_docs": [
                (doc_id, score, title) for doc_id, score, _, title in retrieved_docs
            ],
            "retrieval_count": len(retrieved_docs),
            "bm25_retrieval_time": bm25_time,
            "llm_generation_time": llm_generation_time,
            "avg_bm25_score": avg_score,
            "selected_model": selected_model,
        }


def main():
    """æµ‹è¯•ç›´æ¥RAG Agent"""
    print("ğŸš€ Testing Direct RAG Agent with 100 Wikipedia documents")
    print("=" * 60)

    # åˆå§‹åŒ–Agent
    agent = DirectRAGAgent(
        documents_json_path="wikipedia_documents.json", bm25_binary_path="/home/yimu3/approxMLIRTuner/examples/LLM_bm25/binary/bm25.exec"
    )

    # ä»dataset JSONæ–‡ä»¶åŠ è½½é—®é¢˜
    print("ğŸ“– Loading questions from enhanced_custom_dataset.json...")
    with open("enhanced_custom_dataset.json", "r", encoding="utf-8") as f:
        dataset = json.load(f)

    test_questions = dataset["questions"]
    expected_answers = dataset["answers"]

    print(f"âœ… Loaded {len(test_questions)} questions from dataset")

    print(f"\nğŸ” Testing {len(test_questions)} questions:")

    # ç»Ÿè®¡æ­£ç¡®ç‡å’Œå­˜å‚¨æ‰€æœ‰ç»“æœ
    correct_count = 0
    all_results = []

    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ Question {i}/{len(test_questions)}: {question}")
        print(f"ğŸ¯ Expected Answer: {expected_answers[i-1]}")
        print("-" * 50)

        result = agent.answer_question(question, top_k=5)
        generated_answer = result["answer"]

        # éªŒè¯ç­”æ¡ˆ
        is_correct = check_answer_correct(generated_answer, expected_answers[i - 1])
        if is_correct:
            correct_count += 1
            print(f"âœ… ç­”æ¡ˆæ­£ç¡®!")
        else:
            print(f"âŒ ç­”æ¡ˆä¸æ­£ç¡®")

        print(f"ğŸ¤– Generated Answer: {generated_answer}")
        print(f"â±ï¸  BM25 Retrieval Time: {result['bm25_retrieval_time']:.3f} seconds")
        print(f"â±ï¸  LLM Generation Time: {result['llm_generation_time']:.2f} seconds")
        print(f"ğŸ“Š Retrieved {result['retrieval_count']} documents")

        if result["retrieved_docs"]:
            print("ğŸ“š Retrieved documents:")
            for j, (doc_id, score, title) in enumerate(result["retrieved_docs"]):
                print(f"  {j+1}. {title} (Score: {score:.4f})")

        # å­˜å‚¨æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†ç»“æœ
        question_result = {
            "question_id": i,
            "question": question,
            "expected_answers": expected_answers[i - 1],
            "generated_answer": generated_answer,
            "is_correct": is_correct,
            "bm25_retrieval_time": result["bm25_retrieval_time"],
            "llm_generation_time": result["llm_generation_time"],
            "retrieved_documents": [
                {"rank": j + 1, "doc_id": doc_id, "title": title, "score": score}
                for j, (doc_id, score, title) in enumerate(result["retrieved_docs"])
            ],
            "retrieval_count": result["retrieval_count"],
        }
        all_results.append(question_result)

    # è®¡ç®—æ€»ä½“ç»“æœ
    accuracy = correct_count / len(test_questions)
    total_bm25_time = sum(result["bm25_retrieval_time"] for result in all_results)
    total_llm_time = sum(result["llm_generation_time"] for result in all_results)
    avg_bm25_time = total_bm25_time / len(test_questions) if test_questions else 0
    avg_llm_time = total_llm_time / len(test_questions) if test_questions else 0

    # åˆ›å»ºå®Œæ•´çš„è¯„ä¼°ç»“æœ
    evaluation_results = {
        "metadata": {
            "total_questions": len(test_questions),
            "correct_answers": correct_count,
            "accuracy": accuracy,
            "accuracy_percentage": f"{accuracy:.1%}",
            "total_bm25_time": total_bm25_time,
            "average_bm25_time": avg_bm25_time,
            "total_llm_time": total_llm_time,
            "average_llm_time": avg_llm_time,
            "test_date": "2024-12-19",
            "model": "Gemma:2b",
            "retrieval_method": "BM25",
            "top_k": 3,
        },
        "questions": all_results,
    }

    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    output_file = "rag_evaluation_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)

    # æ‰“å°æ€»ä½“ç»“æœ
    total_time = total_bm25_time + total_llm_time
    print(f"\nğŸ“Š æ€»ä½“è¯„ä¼°ç»“æœ:")
    print(f"  æ€»é—®é¢˜æ•°: {len(test_questions)}")
    print(f"  æ­£ç¡®ç­”æ¡ˆæ•°: {correct_count}")
    print(f"  å‡†ç¡®ç‡: {accuracy:.1%}")
    print(f"  æ€»ç”Ÿæˆæ—¶é—´: {total_time:.3f} ç§’")
    print(f"  æ€»BM25æ£€ç´¢æ—¶é—´: {total_bm25_time:.3f} ç§’")
    print(f"  å¹³å‡BM25æ£€ç´¢æ—¶é—´: {avg_bm25_time:.3f} ç§’")
    print(f"  æ€»LLMç”Ÿæˆæ—¶é—´: {total_llm_time:.2f} ç§’")
    print(f"  å¹³å‡LLMç”Ÿæˆæ—¶é—´: {avg_llm_time:.2f} ç§’")
    print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # è¾“å‡º tuner éœ€è¦çš„æ ¼å¼
    print(f"\nTUNER_OUTPUT: time={total_time:.6f} accuracy={accuracy:.6f}")


if __name__ == "__main__":
    main()
